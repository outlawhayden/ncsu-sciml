{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c99b6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "configuring backend...\n",
      "Configured JAX backend: metal (Apple Silicon)\n",
      "backend selected:\n",
      " METAL\n",
      "active devices:\n",
      " [METAL(id=0)]\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax import vmap\n",
    "import numpy as np\n",
    "import platform\n",
    "import optax\n",
    "import equinox as eqx\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## BACKEND AUTOCONFIG - find GPU if it's there\n",
    "print(\"\\nconfiguring backend...\")\n",
    "system = platform.system()\n",
    "machine = platform.machine().lower()\n",
    "\n",
    "if system == \"Darwin\" and (\"arm\" in machine or \"apple\" in machine or \"m1\" in machine or \"m2\" in machine):\n",
    "    try:\n",
    "        jax.config.update(\"jax_platform_name\", \"METAL\")\n",
    "        print(\"Configured JAX backend: metal (Apple Silicon)\")\n",
    "    except Exception as e:\n",
    "        print(\"Metal not available, falling back to default:\", e)\n",
    "elif system == \"Linux\":\n",
    "    devices = jax.devices()\n",
    "    if any(d.platform == \"gpu\" for d in devices):\n",
    "        jax.config.update(\"jax_platform_name\", \"gpu\")\n",
    "        print(\"Configured JAX backend: gpu\")\n",
    "    else:\n",
    "        jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "        print(\"Configured JAX backend: cpu\")\n",
    "else:\n",
    "    jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "    print(\"Configured JAX backend: cpu\")\n",
    "\n",
    "print(\"backend selected:\\n\", jax.default_backend())\n",
    "print(\"active devices:\\n\", jax.devices())\n",
    "print(\"--------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da5d35",
   "metadata": {},
   "source": [
    "## EQX Module from Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d98ac61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import time\n",
    "import json\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax import vmap, jit, value_and_grad\n",
    "import optax\n",
    "import time\n",
    "\n",
    "def create_FNN(*, key, input_dim, output_dim, width, depth, act_func):\n",
    "    if act_func == 'tanh':\n",
    "        activation = jnp.tanh\n",
    "    if act_func == 'relu':\n",
    "        activation = jax.nn.relu\n",
    "    if act_func == 'swish':\n",
    "        activation = jax.nn.swish\n",
    "    if act_func == 'sine':\n",
    "        activation = jnp.sin\n",
    "    if act_func == 'cosine':\n",
    "        activation = jnp.cos\n",
    "    if act_func == 'gelu':\n",
    "        activation = jax.nn.gelu\n",
    "    return eqx.nn.MLP(in_size=input_dim, out_size=output_dim, width_size=width, depth=depth, activation=activation, key=key)\n",
    "\n",
    "def save_MODEL(filename, hyperparams, model):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        hyperparam_str = json.dumps(hyperparams)\n",
    "        f.write((hyperparam_str + \"\\n\").encode())\n",
    "        eqx.tree_serialise_leaves(f, model)\n",
    "\n",
    "def load_FNN(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        hyperparams = json.loads(f.readline().decode())\n",
    "        model = create_FNN(key=jr.PRNGKey(0), **hyperparams)\n",
    "        return eqx.tree_deserialise_leaves(f, model)\n",
    "\n",
    "def create_OPTIMIZER(optim_hyperparams):\n",
    "    LR0   = optim_hyperparams['LEARNING_RATE_INITIAL']\n",
    "    if optim_hyperparams['LEARNING_RATE_SCHEDULER'] == 'exponential':\n",
    "        STEP  = optim_hyperparams['LEARNING_RATE_STEP']\n",
    "        DECAY = optim_hyperparams['LEARNING_RATE_DECAY']\n",
    "        LEARNING_RATE = optax.schedules.exponential_decay(LR0, STEP, DECAY)\n",
    "    else:\n",
    "        LEARNING_RATE = LR0\n",
    "    \n",
    "    OPTIM_NAME = optim_hyperparams['NAME']\n",
    "    print(f'Selected Optimizer is [ {OPTIM_NAME} ], Initial Learning Rate is {LR0:1.2e}')\n",
    "    if optim_hyperparams['LEARNING_RATE_SCHEDULER'] == 'exponential':\n",
    "        print(f'You are using [ exponential ] learning rate scheduler with rate: {DECAY:.2f} and step: {STEP}')\n",
    "    else:\n",
    "        print(f'You are using [ constant ] learning rate')\n",
    "            \n",
    "    if OPTIM_NAME == 'adabelief':\n",
    "        optimizer = optax.adabelief(LEARNING_RATE)\n",
    "    if OPTIM_NAME == 'adadelta':\n",
    "        optimizer = optax.adadelta(LEARNING_RATE)\n",
    "    if OPTIM_NAME == 'adan':\n",
    "        optimizer = optax.adan(LEARNING_RATE)\n",
    "    if OPTIM_NAME == 'adafactor':\n",
    "        optimizer = optax.adafactor(LEARNING_RATE)\n",
    "    if OPTIM_NAME == 'adagrad':\n",
    "        optimizer = optax.adagrad(LEARNING_RATE) \n",
    "    if OPTIM_NAME == 'adam':\n",
    "        optimizer = optax.adam(LEARNING_RATE) \n",
    "    if OPTIM_NAME == 'adamw':\n",
    "        optimizer = optax.adamw(LEARNING_RATE) \n",
    "    if OPTIM_NAME == 'adamax':\n",
    "        optimizer = optax.adamax(LEARNING_RATE) \n",
    "    if OPTIM_NAME == 'adamaxw':\n",
    "        optimizer = optax.adamaxw(LEARNING_RATE) \n",
    "    if OPTIM_NAME == 'amsgrad':\n",
    "        optimizer = optax.amsgrad(LEARNING_RATE) \n",
    "    if OPTIM_NAME == 'lion':\n",
    "        optimizer = optax.lion(LEARNING_RATE) \n",
    "    if OPTIM_NAME == 'nadam':\n",
    "        optimizer = optax.nadam(LEARNING_RATE)\n",
    "    if OPTIM_NAME == 'nadamw':\n",
    "        optimizer = optax.nadamw(LEARNING_RATE) \n",
    "    if OPTIM_NAME == 'novograd':\n",
    "        optimizer = optax.novograd(LEARNING_RATE) \n",
    "    if OPTIM_NAME == 'polyak_sgd':\n",
    "        optimizer = optax.polyak_sgd(LEARNING_RATE)\n",
    "    if OPTIM_NAME == 'radam':\n",
    "        optimizer = optax.radam(LEARNING_RATE) \n",
    "    if OPTIM_NAME == 'rmsprop':\n",
    "        optimizer = optax.rmsprop(LEARNING_RATE)\n",
    "    if OPTIM_NAME == 'sgd':\n",
    "        optimizer = optax.sgd(LEARNING_RATE) \n",
    "    if OPTIM_NAME == 'sm3':\n",
    "        optimizer = optax.sm3(LEARNING_RATE)\n",
    "    if OPTIM_NAME == 'yogi':\n",
    "        optimizer = optax.yogi(LEARNING_RATE) \n",
    "         \n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def training_MODEL(model, custom_loss_fn, HYPER_OPTIM):\n",
    "  \n",
    "  MAXITER     = HYPER_OPTIM['MAXITER']\n",
    "  PRINT_EVERY = HYPER_OPTIM['PRINT_EVERY']\n",
    "  optimizer  = create_OPTIMIZER(HYPER_OPTIM)\n",
    "  opt_state  = optimizer.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "\n",
    "  @eqx.filter_value_and_grad\n",
    "  def loss_fn(model):\n",
    "    return custom_loss_fn(model)\n",
    "\n",
    "  @eqx.filter_jit\n",
    "  def make_step(model, opt_state):\n",
    "    lvalue, grads = loss_fn(model)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return lvalue, model, opt_state\n",
    "\n",
    "  log_loss  = []\n",
    "  log_minloss = []\n",
    "  print(25*'-' + 'TRAINING STARTS' + 25*'-')\n",
    "  print(65*'-')\n",
    "  start_time = time.time()\n",
    "  for epoch in range(MAXITER):\n",
    "      current_loss, model, opt_state = make_step(model, opt_state)\n",
    "      log_loss.append(current_loss)\n",
    "      if epoch == 0:\n",
    "          log_minloss.append(current_loss)\n",
    "          model_opt = model\n",
    "      else:\n",
    "          if current_loss < log_minloss[-1]:\n",
    "            minloss = current_loss\n",
    "            model_opt = model\n",
    "          else:\n",
    "            minloss = log_minloss[-1]\n",
    "          log_minloss.append(minloss)\n",
    "          \n",
    "      if (epoch == 0) or (epoch % PRINT_EVERY == 0):\n",
    "          end_time = time.time()\n",
    "          if PRINT_EVERY >= 1000:\n",
    "            print(f\"Epoch {epoch//1000:3d}K: loss = {current_loss:.4e}, minloss = {log_minloss[-1]:.4e}, Time: {end_time-start_time:.2f}s\")\n",
    "          else:\n",
    "            print(f\"Epoch {epoch:4d}: loss = {current_loss:.4e}, minloss = {log_minloss[-1]:.4e}, Time: {end_time-start_time:.2f}s\")\n",
    "          start_time = time.time()\n",
    "\n",
    "  current_loss, _ = loss_fn(model)\n",
    "  log_loss.append(current_loss)        \n",
    "  if current_loss < log_minloss[-1]:\n",
    "    minloss = current_loss\n",
    "    model_opt = model\n",
    "  else:\n",
    "    minloss = log_minloss[-1]\n",
    "  log_minloss.append(minloss)\n",
    "  end_time = time.time()\n",
    "  if PRINT_EVERY >= 1000:\n",
    "    print(f\"Epoch {MAXITER//1000:3d}K: loss = {current_loss:.4e}, minloss = {log_minloss[-1]:.4e}, Time: {end_time-start_time:.2f}s\")\n",
    "  else:\n",
    "    print(f\"Epoch {MAXITER:4d}: loss = {current_loss:.4e}, minloss = {log_minloss[-1]:.4e}, Time: {end_time-start_time:.2f}s\")\n",
    "  \n",
    "  \n",
    "  if HYPER_OPTIM['LBFGS']['USE'] == 'on':\n",
    "      params, static = eqx.partition(model_opt, eqx.is_array)\n",
    "      optim = optax.lbfgs()\n",
    "      opt_state = optim.init(params)\n",
    "    \n",
    "      @eqx.filter_jit\n",
    "      def loss_fn_LBFGS(params):\n",
    "        model = eqx.combine(params, static)\n",
    "        return custom_loss_fn(model)\n",
    "\n",
    "      @eqx.filter_jit\n",
    "      def make_step_bfgs(params, opt_state):\n",
    "        loss, grads = eqx.filter_value_and_grad(loss_fn_LBFGS)(params)\n",
    "\n",
    "        updates, opt_state = optim.update(\n",
    "                grads, \n",
    "                opt_state,\n",
    "                params,\n",
    "                value=loss,\n",
    "                grad=grads,\n",
    "                value_fn=loss_fn_LBFGS\n",
    "            )\n",
    "\n",
    "        params = eqx.apply_updates(params, updates)\n",
    "        return loss, params, opt_state\n",
    "    \n",
    "      # Training Loop \n",
    "      MAXITER     = HYPER_OPTIM['LBFGS']['MAXITER']\n",
    "      PRINT_EVERY = HYPER_OPTIM['LBFGS']['PRINT_EVERY']\n",
    "      print(65*'-')\n",
    "      print(21*'-' + '[LBFGS] TRAINING STARTS' + 21*'-')\n",
    "      print(25*'-' + f' MAXITER= {MAXITER} ' + 25*'-')\n",
    "      start_time  = time.time()\n",
    "      for epoch in range(MAXITER):\n",
    "        model = eqx.combine(params, static)\n",
    "        current_loss, params, opt_state = make_step_bfgs(params, opt_state)\n",
    "        log_loss.append(current_loss)\n",
    "        if epoch == 0:\n",
    "          log_minloss.append(current_loss)\n",
    "          model_opt = model\n",
    "        else:\n",
    "          if current_loss < log_minloss[-1]:\n",
    "            minloss = current_loss\n",
    "            model_opt = model\n",
    "          else:\n",
    "            minloss = log_minloss[-1]\n",
    "          log_minloss.append(minloss)\n",
    "          \n",
    "        if (epoch == 0) or (epoch % PRINT_EVERY == 0):\n",
    "          end_time = time.time()\n",
    "          if PRINT_EVERY >= 1000:\n",
    "             print(f\"[LBFGS] Ep {epoch//1000:3d}K: loss = {current_loss:.2e}, minloss = {log_minloss[-1]:.2e}, Time: {end_time-start_time:.2f}s\")\n",
    "          else:\n",
    "             print(f\"[LBFGS] Ep {epoch:3d}: loss = {current_loss:.2e}, minloss = {log_minloss[-1]:.2e}, Time: {end_time-start_time:.2f}s\")\n",
    "          start_time = time.time()\n",
    "          \n",
    "      model = eqx.combine(params, static)\n",
    "      current_loss, _ = loss_fn(model)\n",
    "      log_loss.append(current_loss)        \n",
    "      if current_loss < log_minloss[-1]:\n",
    "          minloss = current_loss\n",
    "          model_opt = model\n",
    "      else:\n",
    "          minloss = log_minloss[-1]\n",
    "          log_minloss.append(minloss)\n",
    "      end_time = time.time()\n",
    "      if PRINT_EVERY >= 1000:\n",
    "          print(f\"[LBFGS] Ep {MAXITER//1000:3d}K: loss = {current_loss:.2e}, minloss = {log_minloss[-1]:.2e}, Time: {end_time-start_time:.2f}s\")\n",
    "      else:\n",
    "          print(f\"[LBFGS] Ep{MAXITER:3d}: loss = {current_loss:.2e}, minloss = {log_minloss[-1]:.2e}, Time: {end_time-start_time:.2f}s\")\n",
    "    \n",
    "  return model_opt, log_loss, log_minloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3566f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 22\n",
    "np.random.seed(seed)\n",
    "key = jax.random.key(seed)\n",
    "input_dim = 2 #(x,t)\n",
    "output_dim = 1 #u\n",
    "width = 20\n",
    "depth = 9\n",
    "activation_fn = 'tanh'\n",
    "nu = 0.01/jnp.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "123e2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_adam = 4e-4\n",
    "adam_iter = 16000\n",
    "lbfgs_iter = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c9af6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"data/uniform_2000.npz\")\n",
    "xt_bc, xt_ic, xt_re = data[\"xt_bc\"], data[\"xt_ic\"], data[\"xt_re\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fbda036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(eqx.Module):\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, key):\n",
    "        wkey, bkey = jax.random.split(key)\n",
    "        self.weight = jax.random.normal(wkey, (out_size, in_size))\n",
    "        self.bias = jax.random.normal(bkey, (out_size))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.weight @ x + self.bias\n",
    "\n",
    "\n",
    "class MLP(eqx.Module):\n",
    "    layers: list\n",
    "    activations: list\n",
    "\n",
    "    def __init__(self, architecture, key, activation=jax.nn.relu):\n",
    "        keys = jax.random.split(key, len(architecture) - 1)\n",
    "        self.layers = [\n",
    "            Linear(architecture[i], architecture[i+1], keys[i])\n",
    "            for i in range(len(architecture) - 1)\n",
    "        ]\n",
    "        self.activations = [activation] * (len(self.layers) - 1) + [eqx.nn.Identity()]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer, act in zip(self.layers, self.activations):\n",
    "            x = act(layer(x))\n",
    "        return x\n",
    "\n",
    "arch = [[input_dim] + [width] * depth  + [output_dim]]\n",
    "fcnn = MLP(arch, key, activation = jax.nn.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b831bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a07ba17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FROM CLASS EXAMPLE\n",
    "\n",
    "def u_ic(x,t): # initial state\n",
    "    return -1 * jnp.sin(jnp.pi*x)\n",
    "\n",
    "def u_bc(x,t): # 0 dirichlet bcs\n",
    "    return 0\n",
    "\n",
    "def g_BC(xt):\n",
    "    return u_bc(xt[0],xt[1])\n",
    "\n",
    "def g_IC(xt):\n",
    "    return u_ic(xt[0],xt[1])\n",
    "\n",
    "def loss_physics(model):\n",
    "    u     = lambda x,t: model(jnp.stack([x, t]))[0]\n",
    "    dx_u  = lambda x,t: jax.grad(u,argnums=0)(x,t)\n",
    "    dxx_u = lambda x,t: jax.grad(dx_u,argnums=0)(x,t)\n",
    "    dt_u  = lambda x,t: jax.grad(u,argnums=1)(x,t)\n",
    "    eq    = lambda xt: dt_u(xt[0],xt[1]) + u(xt[0], xt[1]) * dx_u(xt[0], xt[1]) - nu * dxx_u(xt[0], xt[1]) # swapped for burger's eqn\n",
    "    return eq\n",
    "\n",
    "def loss_fn(model):\n",
    "    eq = loss_physics(model)\n",
    "    residual = vmap(eq)(xt_re)\n",
    "    bc = jax.vmap(model)(xt_bc) - jax.vmap(g_BC)(xt_bc)[:,None]\n",
    "    ic = jax.vmap(model)(xt_ic) - jax.vmap(g_IC)(xt_ic)[:,None]\n",
    "    return jnp.mean(residual**2) + jnp.mean(bc**2) + jnp.mean(ic**2)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def train_step(model, opt_state, x, y, optimizer):\n",
    "    loss, grads = eqx.filter_value_and_grad(loss_fn)(model, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "@eqx.filter_jit\n",
    "def eval_step(model, x, y):\n",
    "    return loss_fn(model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca1728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m start_time = time.time()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(adam_iter)):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     model, opt_state, train_loss = train_step(\u001b[43mmodel\u001b[49m, opt_state, X_train, y_train, optimizer)\n\u001b[32m     10\u001b[39m end_time = time.time()\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "key = jr.PRNGKey(seed)\n",
    "key, train_key = jr.split(key, num=2)\n",
    "\n",
    "optimizer = optax.adam(lr_adam)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(adam_iter)):\n",
    "    fcnn, opt_state, train_loss = train_step(fcnn, opt_state, X_train, y_train, optimizer)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a23f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
